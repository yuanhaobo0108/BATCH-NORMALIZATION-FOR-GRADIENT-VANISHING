{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuanhaobo0108/BATCH-NORMALIZATION-FOR-GRADIENT-VANISHING/blob/main/BN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQeG2Ssx1ML6"
      },
      "source": [
        "# **Batch normalization for gradient vanishing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNoH58aKUPhC"
      },
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./mnist', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./mnist', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_DUr2Rnudt1"
      },
      "source": [
        "# functions to show an image\n",
        "def imshow(img):\n",
        "    img = img / 0.1307 + 0.3081     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQrHeW8HudwD"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 1 input image channel, 25 output channels, 5x5 square convolution kernel, (2, 2) stride\n",
        "        self.conv1 = nn.Conv2d(1, 25, 12, stride=2)\n",
        "        # 25 input image channel, 64 output channels, 5x5 square convolution kernel, (1, 1) stride, 2 padding\n",
        "        self.conv2 = nn.Conv2d(25, 64, 5, padding=2)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(1024, 64)\n",
        "        self.dp  = nn.Dropout(p=0)\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = torch.flatten(x,1)\n",
        "        x = F.relu(self.fc1(x))  \n",
        "        x = self.dp(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.log_softmax(x, 1)\n",
        "        return x\n",
        "\n",
        "net = Net()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRo2FXz8ysQn"
      },
      "source": [
        "WithoutBN+0.001LR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLAc5gBWudyq"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "loss1 = []\n",
        "acc1=[]\n",
        "for epoch in range(10):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 50 == 49:    # print every 50 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss /50))\n",
        "            \n",
        "            loss1.append(running_loss / 50)\n",
        "\n",
        "            running_loss = 0.0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            #acc1=[]\n",
        "            # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "            with torch.no_grad():\n",
        "                for data in testloader:\n",
        "                    images, labels = data\n",
        "                    # calculate outputs by running images through the network\n",
        "                    outputs = net(images)\n",
        "                    # the class with the highest energy is what we choose as prediction\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()\n",
        "\n",
        "            print('Accuracy of the network on the 10000 test images: %f %%' % (100 * correct / total))\n",
        "            acc1.append(100 * correct / total)  \n",
        "print('Finished Training')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C1uafUKyy2t"
      },
      "source": [
        "WithoutBN+0.01LR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYKzfUnp_RCi"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 1 input image channel, 25 output channels, 5x5 square convolution kernel, (2, 2) stride\n",
        "        self.conv1 = nn.Conv2d(1, 25, 12, stride=2)\n",
        "        # 25 input image channel, 64 output channels, 5x5 square convolution kernel, (1, 1) stride, 2 padding\n",
        "        self.conv2 = nn.Conv2d(25, 64, 5, padding=2)\n",
        "        self.bnorm1 = nn.BatchNorm2d(25)\n",
        "        self.bnorm2 = nn.BatchNorm2d(64)\n",
        "        self.bnorm3 = nn.BatchNorm1d(64)\n",
        "        self.bnorm4 = nn.BatchNorm1d(10)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(1024, 64)\n",
        "        self.dp  = nn.Dropout(p=0)\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bnorm1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bnorm2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = torch.flatten(x,1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.bnorm3(x)\n",
        "        x = F.relu(x)  \n",
        "        x = self.dp(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.bnorm4(x)\n",
        "        x = F.log_softmax(x, 1)\n",
        "        return x\n",
        "\n",
        "net = Net()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwJ9dRJoy1ON"
      },
      "source": [
        "WithBN+0.001LR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4guYLuZueKl"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "loss3 = []\n",
        "acc3=[]\n",
        "for epoch in range(10):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 50 == 49:    # print every 50 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 50))\n",
        "            \n",
        "            loss3.append(running_loss / 50)\n",
        "\n",
        "            running_loss = 0.0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            #acc1=[]\n",
        "            # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "            with torch.no_grad():\n",
        "                for data in testloader:\n",
        "                    images, labels = data\n",
        "                    # calculate outputs by running images through the network\n",
        "                    outputs = net(images)\n",
        "                    # the class with the highest energy is what we choose as prediction\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()\n",
        "\n",
        "            print('Accuracy of the network on the 10000 test images: %f %%' % (100 * correct / total))\n",
        "            acc3.append(100 * correct / total)  \n",
        "print('Finished Training')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yFXaTWjy4Gl"
      },
      "source": [
        "WithBN+0.01LR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tg5gZpzu_hF3"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
        "loss4 = []\n",
        "acc4=[]\n",
        "for epoch in range(10):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 50 == 49:    # print every 50 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 50))\n",
        "            \n",
        "            loss4.append(running_loss / 50)\n",
        "\n",
        "            running_loss = 0.0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            #acc1=[]\n",
        "            # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "            with torch.no_grad():\n",
        "                for data in testloader:\n",
        "                    images, labels = data\n",
        "                    # calculate outputs by running images through the network\n",
        "                    outputs = net(images)\n",
        "                    # the class with the highest energy is what we choose as prediction\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()\n",
        "\n",
        "            print('Accuracy of the network on the 10000 test images: %f %%' % (100 * correct / total))\n",
        "            acc4.append(100 * correct / total)  \n",
        "print('Finished Training')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HymOFSnej9rn"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9)\n",
        "loss2 = []\n",
        "acc2=[]\n",
        "for epoch in range(10):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 50 == 49:    # print every 50 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 50))\n",
        "            \n",
        "            loss2.append(running_loss / 50)\n",
        "\n",
        "            running_loss = 0.0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            #acc1=[]\n",
        "            # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "            with torch.no_grad():\n",
        "                for data in testloader:\n",
        "                    images, labels = data\n",
        "                    # calculate outputs by running images through the network\n",
        "                    outputs = net(images)\n",
        "                    # the class with the highest energy is what we choose as prediction\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()\n",
        "\n",
        "            print('Accuracy of the network on the 10000 test images: %f %%' % (100 * correct / total))\n",
        "            acc2.append(100 * correct / total)  \n",
        "print('Finished Training')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZTqbe5S-bpG"
      },
      "source": [
        "plt.plot(loss1,  label = \"withoutBN+0.001LR\")\n",
        "plt.plot(loss3,  label = \"withBN+0.001LR\")\n",
        "plt.plot(loss4,  label = \"withBN+0.01LR\")\n",
        "plt.plot(loss2,  label = \"withBN+0.1LR\")\n",
        "plt.xlabel('Every 50 Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmCV3Tre-bzC"
      },
      "source": [
        "plt.plot(acc1,  label = \"withoutBN+0.001LR\")\n",
        "plt.plot(acc3,  label = \"withBN+0.001LR\")\n",
        "plt.plot(acc4,  label = \"withBN+0.01LR\")\n",
        "plt.plot(acc2,  label = \"withBN+0.1LR\")\n",
        "plt.xlabel('Every 50 Steps')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UGEaXSFClID"
      },
      "source": [
        "DP=0.2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hg_gh_7EjLe7"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 1 input image channel, 25 output channels, 5x5 square convolution kernel, (2, 2) stride\n",
        "        self.conv1 = nn.Conv2d(1, 25, 12, stride=2)\n",
        "        # 25 input image channel, 64 output channels, 5x5 square convolution kernel, (1, 1) stride, 2 padding\n",
        "        self.conv2 = nn.Conv2d(25, 64, 5, padding=2)\n",
        "        self.bnorm1 = nn.BatchNorm2d(25)\n",
        "        self.bnorm2 = nn.BatchNorm2d(64)\n",
        "        self.bnorm3 = nn.BatchNorm1d(64)\n",
        "        self.bnorm4 = nn.BatchNorm1d(10)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(1024, 64)\n",
        "        self.dp  = nn.Dropout(p=0.2)\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bnorm1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bnorm2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = torch.flatten(x,1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.bnorm3(x)\n",
        "        x = F.relu(x)  \n",
        "        x = self.dp(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.bnorm4(x)\n",
        "        x = F.log_softmax(x, 1)\n",
        "        return x\n",
        "\n",
        "net = Net()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8H51zPjjLhv"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
        "loss5 = []\n",
        "acc5=[]\n",
        "for epoch in range(10):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 50 == 49:    # print every 50 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 50))\n",
        "            \n",
        "            loss5.append(running_loss / 50)\n",
        "\n",
        "            running_loss = 0.0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            #acc1=[]\n",
        "            # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "            with torch.no_grad():\n",
        "                for data in testloader:\n",
        "                    images, labels = data\n",
        "                    # calculate outputs by running images through the network\n",
        "                    outputs = net(images)\n",
        "                    # the class with the highest energy is what we choose as prediction\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()\n",
        "\n",
        "            print('Accuracy of the network on the 10000 test images: %f %%' % (100 * correct / total))\n",
        "            acc5.append(100 * correct / total)  \n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4N6WM3CCzZT"
      },
      "source": [
        "Dp=0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk8sWwJGjLkW"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 1 input image channel, 25 output channels, 5x5 square convolution kernel, (2, 2) stride\n",
        "        self.conv1 = nn.Conv2d(1, 25, 12, stride=2)\n",
        "        # 25 input image channel, 64 output channels, 5x5 square convolution kernel, (1, 1) stride, 2 padding\n",
        "        self.conv2 = nn.Conv2d(25, 64, 5, padding=2)\n",
        "        self.bnorm1 = nn.BatchNorm2d(25)\n",
        "        self.bnorm2 = nn.BatchNorm2d(64)\n",
        "        self.bnorm3 = nn.BatchNorm1d(64)\n",
        "        self.bnorm4 = nn.BatchNorm1d(10)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(1024, 64)\n",
        "        self.dp  = nn.Dropout(p=0.5)\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bnorm1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bnorm2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = torch.flatten(x,1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.bnorm3(x)\n",
        "        x = F.relu(x)  \n",
        "        x = self.dp(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.bnorm4(x)\n",
        "        x = F.log_softmax(x, 1)\n",
        "        return x\n",
        "\n",
        "net = Net()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxTZTwhOjLm_"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
        "loss6 = []\n",
        "acc6=[]\n",
        "for epoch in range(10):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 50 == 49:    # print every 50 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 50))\n",
        "            \n",
        "            loss6.append(running_loss / 50)\n",
        "\n",
        "            running_loss = 0.0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            #acc1=[]\n",
        "            # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "            with torch.no_grad():\n",
        "                for data in testloader:\n",
        "                    images, labels = data\n",
        "                    # calculate outputs by running images through the network\n",
        "                    outputs = net(images)\n",
        "                    # the class with the highest energy is what we choose as prediction\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()\n",
        "\n",
        "            print('Accuracy of the network on the 10000 test images: %f %%' % (100 * correct / total))\n",
        "            acc6.append(100 * correct / total)  \n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZtx8vgMjLpu"
      },
      "source": [
        "plt.plot(loss4,  label = \"withBN+0Dropout\")\n",
        "plt.plot(loss5,  label = \"withBN+0.2Dropout\")\n",
        "plt.plot(loss6,  label = \"withBN+0.5Dropout\")\n",
        "plt.xlabel('Every 50 Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeOZpwAejLsT"
      },
      "source": [
        "plt.plot(acc4,  label = \"withBN+0Dropout\")\n",
        "plt.plot(acc5,  label = \"withBN+0.2Dropout\")\n",
        "plt.plot(acc6,  label = \"withBN+0.5Dropout\")\n",
        "plt.xlabel('Every 50 Steps')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNjCUNS1YdX_"
      },
      "source": [
        "BN after activation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNxrE0UlJ9dd"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 1 input image channel, 25 output channels, 5x5 square convolution kernel, (2, 2) stride\n",
        "        self.conv1 = nn.Conv2d(1, 25, 12, stride=2)\n",
        "        # 25 input image channel, 64 output channels, 5x5 square convolution kernel, (1, 1) stride, 2 padding\n",
        "        self.conv2 = nn.Conv2d(25, 64, 5, padding=2)\n",
        "        self.bnorm1 = nn.BatchNorm2d(25)\n",
        "        self.bnorm2 = nn.BatchNorm2d(64)\n",
        "        self.bnorm3 = nn.BatchNorm1d(64)\n",
        "        self.bnorm4 = nn.BatchNorm1d(10)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(1024, 64)\n",
        "        self.dp  = nn.Dropout(p=0)\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.bnorm1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)  \n",
        "        x = self.bnorm2(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = torch.flatten(x,1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)  \n",
        "        x = self.bnorm3(x)\n",
        "        x = self.dp(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.log_softmax(x, 1)\n",
        "        x = self.bnorm4(x)\n",
        "        return x\n",
        "\n",
        "net = Net()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vj4h7k83J9X3"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
        "loss7 = []\n",
        "acc7=[]\n",
        "for epoch in range(10):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 50 == 49:    # print every 50 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 50))\n",
        "            \n",
        "            loss7.append(running_loss / 50)\n",
        "\n",
        "            running_loss = 0.0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            #acc1=[]\n",
        "            # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "            with torch.no_grad():\n",
        "                for data in testloader:\n",
        "                    images, labels = data\n",
        "                    # calculate outputs by running images through the network\n",
        "                    outputs = net(images)\n",
        "                    # the class with the highest energy is what we choose as prediction\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()\n",
        "\n",
        "            print('Accuracy of the network on the 10000 test images: %f %%' % (100 * correct / total))\n",
        "            acc7.append(100 * correct / total)  \n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeWZrfByJ9Ne"
      },
      "source": [
        "plt.plot(loss4,  label = \"withBN+before Activation\")\n",
        "plt.plot(loss7,  label = \"withBN+after Activation\")\n",
        "plt.xlabel('Every 50 Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M56S_UJSJ9FH"
      },
      "source": [
        "plt.plot(acc4,  label = \"withBN+before Activation\")\n",
        "plt.plot(acc7,  label = \"withBN+after Activation\")\n",
        "plt.xlabel('Every 50 Steps')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-39Kbj1J86_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}